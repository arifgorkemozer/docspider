# DocSpider: a Dataset of Cross-Domain Natural Language Querying for MongoDB

#### Arif Görkem Özer, Fırat Çekinel, Pınar Karagöz, İsmail Hakkı Toroslu

## Overview

This repository contains a dataset, called DocSpider, for the task of converting natural language (English) to MongoDB querying language (MQL).

Among the files, you can find the scripts developed to test LLM-generated MQLs against the DocSpider dataset. Scripts are mainly compatible to work with OpenAI's large language models (e.g GPT3.5, GPT4); however, one can provide the predicted MQLs generated by other LLMs to see the execution accuracy on the DocSpider test dataset.

## Requirements for scripts

## Install Packages
```
pip install -r requirements.txt
```

#### 2) Spider dataset folder

Also, DocSpider dataset is generated by using the Spider dataset published in [this paper](https://arxiv.org/pdf/1809.08887). You need to have "spider" folder downloaded and it should contain:

- database folder
- tables.json
- train_spider.json
- dev.json
- custom_train_gold.sql (based on the original train_gold.sql)
- custom_dev_gold.sql (based on the original dev_gold.sql)

#### 3) DocSpider dataset folder

You need to have a folder named "docspider_ground_truth_dataset" that contains:

- docspider_ground_truth_dev.csv
- docspider_ground_truth_train.csv
- hardness_dev.csv
- hardness_train.csv

## Test pipeline

python3 nl2query_base_pipeline.py <root_path> <experiment_tag> <experiment_type> <dataset_type>

- add --skip_llm_step for skipping nosql query generation with llm

##### Example run for generating & testing queries using OpenAI models

- Suppose that you want to store test files inside "my_experiments" folder
- Suppose that your experiment tag is "mymodel_experiment"
- Change API key inside step3_llm.py if you want to use OpenAI models for query generation
- Example run should be: 
**python3 nl2query_base_pipeline.py ../my_experiments mymodel_experiment nl2mongo dev**

##### Example run for testing queries using other LLMs

- Suppose that you want to store test files inside "my_experiments" folder
- Suppose that your experiment tag is "mymodel_experiment"
- Create "my_experiments/spider_mongodb_sqlite_comparison/mymodel_experiment" folder
- Prepare "predicted_nosql.tsv" file with MQL queries generated by other LLM. The titles on top should have "query_id" and "pred_nosql". Each line should contain the query id from DocSpider test dataset and the LLMs MQL prediction.
- Put "predicted_nosql.tsv" inside "my_experiments/spider_mongodb_sqlite_comparison/mymodel_experiment"
- Example run should be: 
**python3 nl2query_base_pipeline.py ../my_experiments mymodel_experiment nl2mongo dev --skip-llm-step**


## Fine-tuning LLMs for text-to-NoSQL

You can fine-tune an LLM on text-to-NoSQL using the DocSpider dataset as follows. Note that you need a Wandb account to monitor training logs.

```
python finetune_text2nosql.py \
--model_id "mistralai/Mistral-7B-Instruct-v0.2" # or "deepseek-ai/deepseek-coder-33b-instruct"
--batch 8 \
--llama_prompt True \
--skip_train False \
--lr 2e-5 \
--epoch 3 \
--cache_dir None \
```
