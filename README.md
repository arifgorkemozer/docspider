# DocSpider: a Dataset of Cross-Domain Natural Language Querying for MongoDB

#### Arif Görkem Özer, Fırat Çekinel, Pınar Karagöz, İsmail Hakkı Toroslu

## Overview

This repository contains a dataset, called DocSpider, for the task of converting natural language (English) to MongoDB querying language (MQL).

Among the files, you can find the scripts to benchmark your models. One should provide the predicted MQLs generated by an LLM to see the execution accuracy on the DocSpider test dataset.

## Requirements for the benchmark pipeline

#### 1) Install Packages
```
pip install -r requirements.txt
```

#### 2) Spider dataset directory

Also, DocSpider dataset is generated by using the Spider dataset published in [this paper](https://arxiv.org/pdf/1809.08887). You should download Spider data from [here](https://drive.google.com/file/d/1403EGqzIDoHMdQF4c9Bkyl7dZLZ5Wt6J/view) and extract contents of *spider.zip* as the *spider* directory. Then, execute migrate_spider_data_to_mongodb.py to copy data from Spider .sqlite files into MongoDB collections. 

#### 3) DocSpider dataset directory

You should see a directory named *docspider_ground_truth_dataset* that contains:

- train.json (DocSpider train dataset in JSON format)
- dev.json (DocSpider test dataset in JSON format)
- collections.json (DocSpider dataset collections in JSON format)
- train_gold.tsv (DocSpider train dataset gold MongoDB queries)
- dev_gold.tsv (DocSpider test dataset gold MongoDB queries)
- hardness_train.csv (Query hardness values from Spider train dataset)
- hardness_dev.csv (Query hardness values from Spider test dataset)
- docspider_ground_truth_train.csv (DocSpider train dataset in CSV format)
- docspider_ground_truth_dev.csv (DocSpider test dataset in CSV format)

## Running benchmark pipeline

#### 1) Pipeline arguments

*benchmark_pipeline.py* takes 1 mandatory and 1 optional arguments:

- **experiment_tag:** Name of the experiment of your choice for identification. All query outputs will be stored under "experiments/<experiment_tag>" directory.

- **--skip_execution_step (optional):** If this flag is not provided, the test pipeline will run the predicted and the actual queries on the DocSpider test dataset. To bypass query execution, you should pass this flag.

#### 2) Benchmarking steps

- Firstly, create a directory under *experiments* directory. The name of the new directory will be the "experiment tag".

- Then place your model's predicted MongoDB queries in a file called *predicted_nosql.tsv* and put it inside the new directory you created. This tab-separated file should be in this format:

```
query_id    pred_nosql
1   <predicted MQL for query #1>
2   <predicted MQL for query #2>
3   <predicted MQL for query #3>
...
...
```

- If the new directory name is "mynewtest", then you need to put *predicted_nosql.tsv* under *experiments/mynewtest* directory and run the pipeline script in the following way:

```
cd experiments
mkdir mynewtest
cd mynewtest
touch predicted_nosql.tsv (then follow the format above to prepare the file)
cd ../../pipelines
python3 benchmark_pipeline.py mynewtest
```

- You can add **--skip_execution_step** for skipping MQL query execution.


## Quality levels for generated MQLs

There are 4 quality levels that we identified while preparing the DocSpider dataset. Based on these quality levels, we identified the issues in the generated MQLs and manually corrected wrong MQLs to increase the number of queries we provide in the DocSpider dataset.

- **Same:** Both query results should be identical. If the query results are in different order, but gold SQL does not have order by clause; these are also considered as identical. This is the most strict comparison metric among the four quality levels.
- **Extra fields:** The generated MQL query returns the same rows with additional fields.
- **Unordered:** Both query results must be the same, except they are ordered differently. In other words, the gold SQL statement includes the order by clause and the order of the result set of the generated MQL and gold SQL are different.
- **Unordered extra fields:** The MongoDB query returned extra fields and the query results have different ordering. This quality level is the lowest one acceptable for queries that can be included in the ground truth dataset.

If a query is classified as `extra_fields` (or `unordered_extra_fields`), it means there is a projection error to fix. 

If the MQL query result ordering is different than the SQLite query result ordering, the MQL query is classified as `unordered` (or `unordered_extra_fields`). If the gold SQL query includes `ORDER BY` clause, then it shows that there is an ordering error to fix.

This is why we report 4 different accuracies at the end of the benchmark pipeline. `same` means the lowerbound, `unordered_extra_fields` means the upperbound of the number of queries could be correctly guessed, with correct field selection and correct ordering.


## Fine-tuning LLMs for text-to-NoSQL

You can fine-tune an LLM on text-to-NoSQL using the DocSpider dataset as follows. Note that you need a Wandb account to monitor training logs.

```
python finetune_text2nosql.py \
--model_id "mistralai/Mistral-7B-Instruct-v0.2" # or "deepseek-ai/deepseek-coder-33b-instruct"
--batch 8 \
--llama_prompt True \
--skip_train False \
--lr 2e-5 \
--epoch 3 \
--cache_dir None \
```
