# DocSpider: a Dataset of Cross-Domain Natural Language Querying for MongoDB

#### Arif Görkem Özer, Fırat Çekinel, Pınar Karagöz, İsmail Hakkı Toroslu

## Overview

This repository contains a dataset, called DocSpider, for the task of converting natural language (English) to MongoDB querying language (MQL).

Among the files, you can find the scripts developed to test LLM-generated MQLs against the DocSpider dataset. Scripts are mainly compatible to work with OpenAI's large language models (e.g GPT3.5, GPT4); however, one can provide the predicted MQLs generated by other LLMs to see the execution accuracy on the DocSpider test dataset.

## Requirements for scripts

#### 1) Install Packages
```
pip install -r requirements.txt
```

#### 2) Spider dataset directory

Also, DocSpider dataset is generated by using the Spider dataset published in [this paper](https://arxiv.org/pdf/1809.08887). You should download Spider data from [here](https://drive.google.com/file/d/1403EGqzIDoHMdQF4c9Bkyl7dZLZ5Wt6J/view) and extract contents of *spider.zip* as the *spider* directory.

#### 3) DocSpider dataset directory

You should see a directory named *docspider_ground_truth_dataset* that contains:

- docspider_ground_truth_dev.csv
- docspider_ground_truth_train.csv

## Doing experiments on the DocSpider dev dataset

*nl2query_test_pipeline.py* takes 1 mandatory and 2 optional arguments:

- **experiment_tag:** Name of the experiment of your choice for identification. All query outputs will be stored under "experiments/<experiment_tag>" directory.

- **--skip_llm_step (optional):** If this flag is not provided, the test pipeline will try to use an OpenAI LLM for MQL generation for the DocSpider test dataset. To bypass this behavior and provide MQL predictions from other LLMs, you should pass this flag.

- **--skip_execution_step (optional):** If this flag is not provided, the test pipeline will run the predicted and the actual queries on the DocSpider test dataset. To bypass query execution, you should pass this flag.

#### 1) How to generate predictions using OpenAI LLMs?

- Firstly, change the model name to the model name of your choice for *generate_db_queries_with_llm()* call, in *nl2query_test_pipeline.py*.

- Then change the mocked API key in *step3_llm.py*.

- Run the following command with an experiment tag of your choice:

```
cd pipelines
python3 nl2query_test_pipeline.py <experiment_tag>
```


#### 2) How to test existing predictions from other LLMs?

- Firstly, create a directory under *experiments* directory. The name of the new directory will be the "experiment tag".

- Then place your model's predicted MongoDB queries in a file called *predicted_nosql.tsv* and put it inside the new directory you created. This tab-separated file should be in this format:

```
query_id    pred_nosql
3   <predicted MQL for query #3>
4   <predicted MQL for query #4>
5   <predicted MQL for query #5>
...
...
```

- If the new directory name is "mynewtest", then you need to put *predicted_nosql.tsv* under *experiments/mynewtest* directory and run the pipeline script in the following way:

```
cd experiments
mkdir mynewtest
cd mynewtest
touch predicted_nosql.tsv (then follow the format above to prepare the file)
cd ../../pipelines
python3 nl2query_test_pipeline.py mynewtest --skip_llm_step
```

- Don't forget to add **--skip_llm_step** for skipping MQL query generation with OpenAI models


## Fine-tuning LLMs for text-to-NoSQL

You can fine-tune an LLM on text-to-NoSQL using the DocSpider dataset as follows. Note that you need a Wandb account to monitor training logs.

```
python finetune_text2nosql.py \
--model_id "mistralai/Mistral-7B-Instruct-v0.2" # or "deepseek-ai/deepseek-coder-33b-instruct"
--batch 8 \
--llama_prompt True \
--skip_train False \
--lr 2e-5 \
--epoch 3 \
--cache_dir None \
```
