# DocSpider: a Dataset of Cross-Domain Natural Language Querying for MongoDB

#### Arif Görkem Özer, Fırat Çekinel, Pınar Karagöz, İsmail Hakkı Toroslu

## Overview

This repository contains a dataset, called DocSpider, for the task of converting natural language (English) to MongoDB querying language (MQL).

Among the files, you can find the scripts developed to test LLM-generated MQLs against the DocSpider dataset. Scripts are mainly compatible to work with OpenAI's large language models (e.g GPT3.5, GPT4); however, one can provide the predicted MQLs generated by other LLMs to see the execution accuracy on the DocSpider test dataset.

## Requirements for scripts

#### 1) Install Packages
```
pip install -r requirements.txt
```

#### 2) Spider dataset directory

Also, DocSpider dataset is generated by using the Spider dataset published in [this paper](https://arxiv.org/pdf/1809.08887). You should download Spider data from [here](https://drive.google.com/file/d/1403EGqzIDoHMdQF4c9Bkyl7dZLZ5Wt6J/view) and extract contents of *spider.zip* as the *spider* directory. Then, execute migrate_spider_data_to_mongodb.py to copy data from Spider .sqlite files into MongoDB collections. 

#### 3) DocSpider dataset directory

You should see a directory named *docspider_ground_truth_dataset* that contains:

- docspider_ground_truth_train.csv (DocSpider train dataset)
- docspider_ground_truth_dev.csv (DocSpider test dataset)
- hardness_train.csv (Query hardness from Spider train dataset)
- hardness_dev.csv (Query hardness from Spider test dataset)

## MQL generation using Spider's train and test dataset 

This is basically how we generated MQLs from Spider's train and test dataset and identified the problems. *nl2query_generation_pipeline.py* takes 2 mandatory and 2 optional arguments:

- **experiment_tag:** Name of the experiment of your choice for identification. All query outputs will be stored under "experiments/<experiment_tag>" directory.

- **dataset_type:** Type of the dataset used from Spider. Options are: [train, dev]. If "train" selected, it converts SQLs in "train_gold.sql" to MQLs. Otherwise, it converts SQLs in "dev_gold.sql" to MQLs.

- **--skip_llm_step (optional):** If this flag is not provided, the test pipeline will try to use an OpenAI LLM for MQL generation for the DocSpider test dataset. To bypass this behavior and provide MQL predictions from other LLMs, you should pass this flag.

- **--skip_execution_step (optional):** If this flag is not provided, the test pipeline will run the predicted and the actual queries on the DocSpider test dataset. To bypass query execution, you should pass this flag.

## Doing experiments on the DocSpider dev dataset

*nl2query_test_pipeline.py* takes 1 mandatory and 2 optional arguments:

- **experiment_tag:** Name of the experiment of your choice for identification. All query outputs will be stored under "experiments/<experiment_tag>" directory.

- **--skip_llm_step (optional):** If this flag is not provided, the test pipeline will try to use an OpenAI LLM for MQL generation for the DocSpider test dataset. To bypass this behavior and provide MQL predictions from other LLMs, you should pass this flag.

- **--skip_execution_step (optional):** If this flag is not provided, the test pipeline will run the predicted and the actual queries on the DocSpider test dataset. To bypass query execution, you should pass this flag.

#### 1) How to generate predictions from Spider's train and test dataset using OpenAI LLMs?

- Firstly, change the model name to the model name of your choice for *generate_db_queries_with_llm()* call, in *nl2query_generation_pipeline.py*.

- Then change the mocked API key in *step3_llm.py*.

- Run the following command with an experiment tag and dataset type of your choice:

```
cd pipelines
python3 nl2query_generation_pipeline.py <experiment_tag> <dataset_type>
```

#### 2) How to generate predictions using OpenAI LLMs?

- Firstly, change the model name to the model name of your choice for *generate_db_queries_with_llm()* call, in *nl2query_test_pipeline.py*.

- Then change the mocked API key in *step3_llm.py*.

- Run the following command with an experiment tag of your choice:

```
cd pipelines
python3 nl2query_test_pipeline.py <experiment_tag>
```


#### 3) How to test existing predictions from other LLMs?

- Firstly, create a directory under *experiments* directory. The name of the new directory will be the "experiment tag".

- Then place your model's predicted MongoDB queries in a file called *predicted_nosql.tsv* and put it inside the new directory you created. This tab-separated file should be in this format:

```
query_id    pred_nosql
3   <predicted MQL for query #3>
4   <predicted MQL for query #4>
5   <predicted MQL for query #5>
...
...
```

- If the new directory name is "mynewtest", then you need to put *predicted_nosql.tsv* under *experiments/mynewtest* directory and run the pipeline script in the following way:

```
cd experiments
mkdir mynewtest
cd mynewtest
touch predicted_nosql.tsv (then follow the format above to prepare the file)
cd ../../pipelines
python3 nl2query_test_pipeline.py mynewtest --skip_llm_step
```

- Don't forget to add **--skip_llm_step** for skipping MQL query generation with OpenAI models


## Quality levels for generated MQLs

There are 4 quality levels that we identified while preparing the DocSpider dataset. Based on these quality levels, we identified the issues in the generated MQLs and manually corrected wrong MQLs to increase the number of queries we provide in the DocSpider dataset.

- **Same:** Both query results should be identical. If the query results are in different order, but gold SQL does not have order by clause; these are also considered as identical. This is the most strict comparison metric among the four quality levels.
- **Extra fields:** The generated MQL query returns the same rows with additional fields.
- **Unordered:** Both query results must be the same, except they are ordered differently. In other words, the gold SQL statement includes the order by clause and the order of the result set of the generated MQL and gold SQL are different.
- **Unordered extra fields:** The MongoDB query returned extra fields and the query results have different ordering. This quality level is the lowest one acceptable for queries that can be included in the ground truth dataset.

If a query is classified as `extra_fields` (or `unordered_extra_fields`), it means there is a projection error to fix. 

If the MQL query result ordering is different than the SQLite query result ordering, the MQL query is classified as `unordered` (or `unordered_extra_fields`). If the gold SQL query includes `ORDER BY` clause, then it shows that there is an ordering error to fix.

This is why we report 4 different accuracies at the end of the pipelines. `same` meant the lowerbound, `unordered_extra_fields` meant the upperbound of the number of queries could be included to the DocSpider dataset.


## Fine-tuning LLMs for text-to-NoSQL

You can fine-tune an LLM on text-to-NoSQL using the DocSpider dataset as follows. Note that you need a Wandb account to monitor training logs.

```
python finetune_text2nosql.py \
--model_id "mistralai/Mistral-7B-Instruct-v0.2" # or "deepseek-ai/deepseek-coder-33b-instruct"
--batch 8 \
--llama_prompt True \
--skip_train False \
--lr 2e-5 \
--epoch 3 \
--cache_dir None \
```
